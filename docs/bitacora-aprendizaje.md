## üß≠ 2025-10-08 ‚Äî Origen del concepto de m√≥dulo de comunicaci√≥n

- La idea del m√≥dulo de env√≠o y recepci√≥n surgi√≥ como una forma de **unificar la comunicaci√≥n interna entre componentes** (backend, frontend, worker, IA, base de datos), evitando tener que definir rutas y autenticaci√≥n por separado en cada uno.
  
- Se plante√≥ que cada servicio pudiera **enviar o recibir JSONs** mediante una interfaz est√°ndar (por ejemplo, `enviar(json, destino="DB")`).
  
- Durante la reflexi√≥n inicial surgi√≥ la duda de si esto **ya exist√≠a como funcionalidad en frameworks** modernos (FastAPI, gRPC, etc.), lo que llev√≥ a investigar si val√≠a la pena desarrollar un sistema propio o integrar herramientas existentes.
  
- Concluido: la idea se mantiene como concepto de arquitectura, pero se **pospone su implementaci√≥n completa** hasta entender con m√°s profundidad la capa de transporte de datos.
  
- Se utiliz√≥ asistencia de IA para **evaluar la viabilidad t√©cnica y las alternativas existentes**, manteniendo la decisi√≥n final en suspenso mientras se estudian fundamentos.





## üß≠ 2025-10-08 ‚Äî Replanteamiento del m√≥dulo de comunicaci√≥n

Durante el dise√±o inicial del sistema se hab√≠a considerado crear un **m√≥dulo de transporte propio**, responsable de manejar el env√≠o y recepci√≥n de datos entre los distintos servicios (backend, worker, IA, etc.).  
La idea era contar con un bloque dedicado a la **negociaci√≥n, cifrado y transferencia** de mensajes entre m√≥dulos, de forma similar a un gestor de red centralizado.

Sin embargo, una duda que quedaba por resolver era la **cantidad de tipos de peticiones** que se realizar√≠an entre servicios.  
Esta se despej√≥ al volver a analizar el funcionamiento de una consulta HTTP: por lo general, **no son los dise√±adores quienes se adaptan a la consulta HTTP**, sino que **la consulta HTTP se adapta al tipo de datos que el servicio necesite enviar o recibir**.  
Por lo tanto, imponer un m√≥dulo r√≠gido de comunicaci√≥n significaba perder esa flexibilidad inherente.

Esto me hizo recordar que en el dise√±o del cliente ya hab√≠a implementado una **b√∫squeda web segura** usando librer√≠as como `requests`, con autenticaci√≥n y manejo de tokens, lo que cumpl√≠a exactamente con el rol que yo esperaba para el m√≥dulo de env√≠o.  
A su vez, record√© los **CTF‚Äôs** en los que hab√≠a trabajado, donde se empleaban **servidores HTTP en escucha** para recibir y responder peticiones, lo que coincid√≠a con la idea de mi m√≥dulo de recepci√≥n.

M√°s adelante, al profundizar en el funcionamiento real de **FastAPI** y **requests**, comprend√≠ por qu√© los desarrolladores **no suelen preocuparse expl√≠citamente por esta capa**:  
su funcionalidad ya est√° resuelta dentro de los frameworks modernos, que gestionan internamente la creaci√≥n de endpoints, el cifrado HTTPS, la autenticaci√≥n y la validaci√≥n de datos.

---

### üß© Conclusi√≥n

El sistema ahora debe pensarse como una **colecci√≥n de servicios que exponen y consumen endpoints**, en lugar de bloques que dependen de un intermediario com√∫n.  
Con esto resuelto, el siguiente paso es **planificar la arquitectura general**, definiendo c√≥mo se comunican los servicios y qu√© informaci√≥n intercambian.

üß± *Este razonamiento llev√≥ directamente a la **Decisi√≥n de Dise√±o #3.***


## üß≠ 2025-10-13 ‚Äî Evaluaci√≥n de autenticaci√≥n y elecci√≥n de API para YouTube

El an√°lisis comenz√≥ intentando definir el **sistema de autenticaci√≥n** con YouTube: c√≥mo obtener permisos del usuario, gestionar tokens y permitir que el sistema acceda a las playlists.  
Inicialmente se asum√≠a que el proceso ser√≠a est√°ndar (flujo OAuth2 con redirecci√≥n y almacenamiento de tokens).  
Sin embargo, r√°pidamente se detect√≥ que **la forma de autenticarse depende por completo de la API utilizada** ‚Äîes decir, la autenticaci√≥n no es un problema independiente, sino una consecuencia de la API elegida.

Esto cambi√≥ la direcci√≥n del an√°lisis: antes de decidir c√≥mo autenticar, era necesario definir **qu√© API** se va a usar.  
A partir de ah√≠, el foco del razonamiento pas√≥ de ‚Äúc√≥mo autenticamos‚Äù a ‚Äúcon qui√©n autenticamos‚Äù.

---

Durante la exploraci√≥n surgieron tres caminos t√©cnicos:

1. **API oficial (YouTube Data API v3)**  
   Se comprob√≥ que ofrece toda la informaci√≥n necesaria con gran precisi√≥n y estabilidad.  
   El problema est√° en sus **cuotas diarias extremadamente limitadas**, que restringen el n√∫mero de operaciones por usuario.  
   Esto vuelve inviable el flujo principal del proyecto ‚Äîmigrar o sincronizar playlists grandes‚Äî, ya que podr√≠a agotarse la cuota tras unas pocas acciones.

2. **APIs no oficiales**  
   Estas librer√≠as replican las funciones b√°sicas de la API oficial sin l√≠mites de cuota y con autenticaciones m√°s simples (cookies o tokens directos).  
   Aunque resuelven el cuello de botella t√©cnico, presentan **riesgos de mantenimiento**: pueden romperse con cambios en YouTube o sufrir bloqueos eventuales.

3. **Enfoque h√≠brido**  
   Se propuso usar la API oficial para tareas de bajo consumo (por ejemplo, b√∫squedas o validaciones) y una API no oficial para operaciones pesadas (creaci√≥n o lectura masiva).  
   Este equilibrio podr√≠a funcionar, pero **aumenta la complejidad estructural** y la cantidad de puntos de falla.

---

Al revisar los casos de uso, se identificaron tres flujos principales:

- **Usuarios con m√∫sica local** que quieren replicar sus playlists en plataformas.  
- **Usuarios que migran playlists entre plataformas.**  
- **Usuarios que comparan o sincronizan playlists equivalentes** entre distintas fuentes (por ejemplo, entre YouTube y una carpeta local).

De los tres, si los primeros dos se hacen en volummenes muy altos de canciones (de 50 a 100)  implicarian un **mayor consumo de peticiones** y es donde las limitaciones de la API oficial se vuelven cr√≠ticas.

---

El razonamiento llev√≥ a una conclusi√≥n importante:  
el sistema de autenticaci√≥n no es un problema aislado, sino **una consecuencia directa de la API seleccionada**.  
La API oficial cumple con todas las funciones, pero **no con el volumen de trabajo necesario**; por tanto, usarla ser√≠a como dejar el sistema ‚Äúfuncional pero d√©bil‚Äù.  
Por otro lado, depender completamente de una API no oficial implicar√≠a riesgos de estabilidad a largo plazo.

De ah√≠ surgi√≥ la idea de dise√±ar una **capa de traducci√≥n modular** entre el sistema y las APIs, de modo que el c√≥digo principal no dependa directamente de una implementaci√≥n espec√≠fica.  
Esto permitir√° cambiar o combinar APIs sin afectar el resto del sistema, reduciendo los costos futuros de mantenimiento o portabilidad.

---

### üß© Conclusi√≥n

La elecci√≥n definitiva de la API **queda abierta**, pero con una direcci√≥n clara:  
- El sistema debe dise√±arse de forma que **pueda cambiar de API con el menor impacto posible**.  
- La autenticaci√≥n se implementar√° **seg√∫n el flujo que exija la API finalmente seleccionada**.  
- Por ahora, se asume que **la API oficial no es viable como √∫nico canal** debido a sus restricciones de cuota, y se mantendr√° abierta la opci√≥n de APIs no oficiales o h√≠bridas.

üß± *Este razonamiento est√° directamente vinculado a la ** üß≠ Marco de Decisi√≥n #1 ‚Äî Integraci√≥n con YouTube (autenticaci√≥n y capa API)**


---



## üß≠ 2025-10-10 ‚Äî Implementaci√≥n y flujo de autenticaci√≥n con Spotify

El an√°lisis y desarrollo se centraron en construir el **flujo de autenticaci√≥n** con Spotify, entendiendo c√≥mo obtener los permisos del usuario y transformar el c√≥digo de autorizaci√≥n en un token de acceso funcional.

Inicialmente se asum√≠a que bastaba con una simple redirecci√≥n desde el servidor Apache hacia la p√°gina de autorizaci√≥n de Spotify. Sin embargo, se detect√≥ que esa redirecci√≥n deb√≠a **incluir informaci√≥n cr√≠tica del flujo OAuth2** (como el client_id y el redirect_uri) y adem√°s **manejar el intercambio del c√≥digo** devuelto por Spotify, por lo que **deb√≠a controlarse desde FastAPI**, no desde Apache.

---

Durante la implementaci√≥n se estableci√≥ el flujo general:

1. **Redirecci√≥n a Spotify**  
   Un m√©todo de FastAPI genera la URL de autorizaci√≥n con los par√°metros requeridos y redirige al usuario hacia Spotify.  
   El usuario concede los permisos necesarios y Spotify redirige de vuelta al servidor con un c√≥digo temporal.

2. **Recepci√≥n del c√≥digo y canje por token**  
   El m√©todo `callback` recibe el c√≥digo de autorizaci√≥n y lo env√≠a al endpoint de Spotify para obtener el token de acceso.  
   Actualmente, solo se est√° manejando el **access token**, pero no el **refresh token** ‚Äîesto qued√≥ registrado como una **deuda t√©cnica** para permitir la renovaci√≥n autom√°tica de sesiones.

3. **Delegaci√≥n del token al worker**  
   Una vez recibido, el token debe ser reenviado a un componente secundario (worker) que actuar√° como intermediario para realizar las solicitudes a Spotify.  
   Esta parte est√° planificada como el siguiente paso en la implementaci√≥n.
---

### üß© Conclusi√≥n

El flujo de autenticaci√≥n con Spotify **ya es funcional**, aunque con limitaciones t√©cnicas por resolver.  
La estructura modular entre FastAPI (autenticaci√≥n) y el worker (operaciones) permitir√° mantener el c√≥digo limpio y flexible ante futuras ampliaciones.

üß± *Este razonamiento est√° directamente vinculado a la **üß≠ Marco de Decisi√≥n #2 ‚Äî Autenticaci√≥n y manejo de tokens en Spotify***

---


## üß≠ 2025-10-10 ‚Äî  Flujo de autenticaci√≥n en YouTube

En an√°lisis previos se hab√≠a establecido que **la autenticaci√≥n depend√≠a directamente de la API elegida**, y que por dise√±o deb√≠a permanecer **desacoplada de la l√≥gica de negocio**, de modo que pudiera reemplazarse f√°cilmente si la API sufr√≠a alteraciones o bloqueos.

Durante esta etapa se examin√≥ con mayor detalle el funcionamiento interno de la autenticaci√≥n en la librer√≠a `ytmusicapi`, descubriendo que el proceso no emplea el flujo OAuth tradicional, sino que se basa en **encabezados (cookies)** que replican el contexto de una sesi√≥n de navegador leg√≠tima.  
A partir de esto se reconstruy√≥ el flujo t√©cnico real:

---

### üîπ 1. Obtenci√≥n del contexto de sesi√≥n

1. El usuario inicia sesi√≥n en YouTube desde su navegador habitual.  
2. Desde esa sesi√≥n se extraen los encabezados y cookies activas (especialmente `SAPISID`, `SID` o sus equivalentes seg√∫n el dominio).  
3. Con esos datos se reconstruye el **contexto de autenticaci√≥n** que YouTube utiliza internamente para validar al usuario.

> En esencia, el sistema **emula una sesi√≥n de navegador ya autenticada**, evitando el flujo OAuth y aprovechando los tokens que el propio navegador mantiene v√°lidos.

---

### üîπ 2. El problema: acceso a cookies y restricciones del navegador

Inicialmente se propuso que el **frontend actuara como puente** entre el backend y YouTube.  
La idea era que el backend enviara sus solicitudes al frontend, y este ‚Äîal estar en la m√°quina del usuario‚Äî las reenviara directamente a YouTube usando su sesi√≥n autenticada.

Sin embargo, se descubri√≥ un obst√°culo cr√≠tico:  
los navegadores **impiden el acceso a las cookies de sesi√≥n de YouTube desde dominios externos**, bloqueando cualquier intento de obtener `SAPISID` o `SID` por pol√≠ticas de seguridad (*Same-Origin Policy* y protecciones `HttpOnly`).  

Esto hac√≠a imposible que el navegador cumpliera ese rol de intermediario directo sin vulnerar la seguridad del entorno.

---

### üîπ 3. Soluci√≥n: cliente intermediario de escritorio

Para superar esta limitaci√≥n se defini√≥ un **cliente intermediario local**, reutilizando el mismo programa encargado de la limpieza de metadatos.  
Este cliente tendr√≠a tres responsabilidades principales:

1. **Extraer** las cookies de sesi√≥n directamente desde el entorno local del usuario.  
2. **Recibir** las solicitudes del backend y complementarlas con los encabezados necesarios para comunicarse con YouTube.  
3. **Reenviar** las peticiones autenticadas a YouTube y **devolver** los resultados al backend.

De este modo, el flujo de autenticaci√≥n se mantiene **plenamente operativo** sin violar las restricciones del navegador, y al mismo tiempo se logra una **integraci√≥n natural entre el cliente local y el sistema web**.

Adem√°s, esta decisi√≥n resolv√≠a otro requisito estructural:  
al ubicar la l√≥gica espec√≠fica de autenticaci√≥n dentro del cliente, la **l√≥gica de negocio del sistema web queda completamente desacoplada**.  
Si en el futuro se requiere cambiar de API o modificar la estrategia de autenticaci√≥n, bastar√° con **actualizar los m√©todos del cliente**, sin afectar el backend ni el frontend.

---

### üîπ 4. Conclusi√≥n del flujo

Lo que comenz√≥ como una limitaci√≥n t√©cnica ‚Äîla imposibilidad del frontend de manipular cookies seguras‚Äî termin√≥ fortaleciendo la arquitectura general.  
La autenticaci√≥n de YouTube se consolid√≥ dentro del cliente de escritorio, otorg√°ndole un **rol central y estructural** dentro del ecosistema del proyecto.

> En resumen, el cliente no solo limpia metadatos: ahora tambi√©n **funciona como puente de autenticaci√≥n y ejecuci√≥n segura** para las operaciones de YouTube, manteniendo el backend y el frontend libres de responsabilidades sensibles.

---
## üß≠ 2025-10-28 ‚Äî  Planificacion de la BD 

**Estado actual:** flujo funcional y justificado estructuralmente.  
**Pendiente:** definir y documentar la **interfaz de comunicaci√≥n** entre el cliente local y el backend, garantizando un manejo encapsulado y seguro de los tokens de sesi√≥n.


###üîπ 1. Planteamiento inicial: rol de la base de datos

El problema que dio origen a este an√°lisis fue la carga masiva de datos en el frontend.
El navegador no pod√≠a manejar simult√°neamente cientos de canciones sin saturarse, por lo que se concluy√≥ que deb√≠a implementarse un sistema de carga por lotes.
Esto deriv√≥ en una cadena l√≥gica:

El frontend solicita lotes de datos seg√∫n el desplazamiento del usuario (scroll).

El backend responde a esas solicitudes recuperando fragmentos espec√≠ficos desde la DB.

Para evitar sobrecargar al worker, el backend requer√≠a acceso directo a la base de datos en calidad de observador, mientras que el worker ser√≠a el √∫nico editor autorizado (agregar, modificar o eliminar registros).

El modelo final posicion√≥ a los cuatro servicios (frontend, backend, worker e IA) como microservicios independientes, comunicados mediante HTTP, y contenedorizados para mantener su aislamiento operativo.

###üîπ 2. Evaluaci√≥n de tecnolog√≠as: SQL vs Redis

En un principio se asumi√≥ un enfoque cl√°sico con SQL.
Sin embargo, al analizar los requisitos del sistema se concluy√≥ que este modelo era ineficiente:

Se necesitaba acceso extremadamente r√°pido en memoria.

La base de datos deb√≠a ser din√°mica y ef√≠mera (los datos se eliminan al finalizar el proceso).

La estructura de los datos ya exist√≠a en forma de diccionarios Python, lo que hac√≠a natural un modelo key‚Äìvalue.

Bajo estos criterios, Redis emergi√≥ como la opci√≥n m√°s coherente.
Adem√°s, se comprendi√≥ que Redis opera pr√°cticamente como un espacio compartido de memoria RAM entre servicios, optimizado para accesos concurrentes.

###üîπ 3. Primer dise√±o estructural

El esquema inicial propuso una organizaci√≥n independiente por fuente:
```python
YTplaylist = {id1: {...}, id2: {...}}
SPplaylist = {id1: {...}, id2: {...}}
LCplaylist = {id1: {...}, id2: {...}}
relations = {YT1: {SPid3, LCid4}, YT3: {LCid5, SPid6}}
```

Luego surgi√≥ una mejora:
a medida que el sistema detectara relaciones entre canciones de distintas fuentes, estas ser√≠an extra√≠das de las playlists originales y trasladadas a versiones sorted, dejando a las listas principales cada vez m√°s ligeras y evitando duplicaciones.
Se mantendr√≠an copias de respaldo (imagen) para recuperaci√≥n ante errores.

Este modelo result√≥ en nueve estructuras activas: tres originales, tres sorted y tres de respaldo, m√°s el arreglo general de relaciones.

###üîπ 4. Riesgo identificado: atomizaci√≥n de estructuras

Durante el montaje m√≠nimo de Redis se explor√≥ la forma de persistir estas estructuras.
Inicialmente se pens√≥ en almacenar una playlist completa como un √∫nico valor de texto (string).
Sin embargo, se detect√≥ un riesgo grave: cualquier corrupci√≥n o error sobre ese string afectar√≠a toda la playlist, y las operaciones parciales implicar√≠an reconvertir un bloque completo de cientos de registros.

La soluci√≥n fue adoptar diccionarios anidados como valores dentro de Redis, permitiendo editar o leer cada canci√≥n individualmente sin convertir estructuras completas.
Esto redujo la complejidad y mejor√≥ la seguridad de las operaciones.

Este hallazgo marc√≥ el punto de inflexi√≥n: Redis no ser√≠a un simple contenedor de strings, sino una base de datos semiestructurada con acceso granular a cada entrada.

###üîπ 5. Optimizaci√≥n de b√∫squeda y relaciones

El siguiente cuello detectado fue el de b√∫squeda de relaciones.
Buscar equivalencias desde una fuente distinta implicaba iteraciones largas sobre estructuras grandes.
Para resolverlo, se dise√±√≥ una indexaci√≥n auxiliar de relaciones, que mapea directamente cada ID de canci√≥n al ID de relaci√≥n correspondiente:

```python
relations = {
  "rel_001": {"equivalents":
    {"yt": "yt123",
    "sp": "sp456"}}
}
relation_index = {
  "yt123": "rel_001",
  "sp456": "rel_001"}
```

Esta decisi√≥n duplic√≥ parcialmente la informaci√≥n, pero redujo las b√∫squedas a dos consultas directas en lugar de una iteraci√≥n masiva, mejorando dr√°sticamente la velocidad.

###üîπ 6. Estandarizaci√≥n y m√©todos b√°sicos

Se unificaron todas las fuentes dentro de una sola estructura jer√°rquica:

```python
playlists = {
    "yt": {...},
    "sp": {...},
    "local": {...}
}
```

Con esta estructura se desarrollaron m√©todos b√°sicos para:

A√±adir canciones.

Obtener canciones por ID.

Crear relaciones entre canciones.

Definir b√∫squedas flexibles (para IA o backend).

Se acord√≥ diferir la construcci√≥n definitiva del formato JSON de b√∫squeda hasta disponer de datos reales provenientes del worker.

###üîπ 7. Consideraciones sobre indexaci√≥n y consumo de memoria

Redis permite consultas avanzadas mediante etiquetas si los metadatos est√°n almacenados como pares clave:valor simples.
Esto llev√≥ a considerar una posible duplicaci√≥n adicional de datos, lo que en teor√≠a aumentar√≠a el consumo de RAM.
Sin embargo, una estimaci√≥n preliminar ‚Äî90000 canciones con 15 metadatos cada una‚Äî arroj√≥ un consumo aproximado de 500 MB, considerado aceptable.

De esta conclusi√≥n emergi√≥ un principio general de dise√±o:

Es preferible evitar bucles de b√∫squeda incluso si eso implica mayor consumo de memoria temporal.

###üîπ 8. Rol del servicio de IA

Durante la simulaci√≥n del comportamiento del asistente se estableci√≥ que la IA requerir√°:

Un entorno Redis local para manipular sus propios filtros.

Comandos de limpieza y reconstrucci√≥n para gestionar sus estructuras auxiliares.

Capacidad para ejecutar operaciones de filtrado y conteo, a partir de las cuales inferir√° resultados o heur√≠sticas.

Se determin√≥ que librer√≠as como Redisearch podr√≠an incorporarse m√°s adelante, aunque las operaciones booleanas nativas de Redis ya ofrecen la flexibilidad necesaria.

###üîπ 9. Estado actual

El sistema de almacenamiento temporal ha sido conceptualizado y estructurado con base en Redis.
Las estructuras principales y los m√©todos de manipulaci√≥n ya est√°n definidos en c√≥digo y listos para pruebas unitarias.
Las decisiones de dise√±o tomadas priorizan acceso directo, modularidad y resiliencia frente a errores.

Pendiente: validar emp√≠ricamente el consumo de memoria real durante operaciones del worker y definir el formato JSON final para las consultas de la IA y el backend.
